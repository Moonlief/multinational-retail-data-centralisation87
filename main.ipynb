{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules and staring local db engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials loaded sucessfully!!\n",
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "#Importing modules\n",
    "\n",
    "import boto3\n",
    "\n",
    "from data_cleaning import DataCleaning\n",
    "from data_extraction import DataExtractor\n",
    "from database_utils import DatabaseConnector\n",
    "import importlib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "import tabula\n",
    "import yaml\n",
    "\n",
    "\n",
    "#starting local database\n",
    "\n",
    "#read creds\n",
    "uploader = DatabaseConnector()\n",
    "ml_yaml = 'ml_dbs.yaml'\n",
    "uploader.read_db_creds(ml_yaml)\n",
    "\n",
    "# initialising and returning an sqlalchemy database engine.\n",
    "\n",
    "uploading = uploader.init_db_engine_postgresql()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDS ETL: User\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Getting data from the RDS database\n",
    "\n",
    "## read credentials\n",
    "connector_rds = DatabaseConnector()\n",
    "ai_core_yaml = 'db_creds.yaml'\n",
    "connector_rds.read_db_creds(ai_core_yaml)\n",
    "\n",
    "## initialising and returning an sqlalchemy database engine.\n",
    "connection_rds = connector_rds.init_db_engine()\n",
    "\n",
    "## Reading the data from the RDS database\n",
    "connector_tables_rds = connector_rds.list_db_tables()\n",
    "\n",
    "\n",
    "### 2. Extracting the 'legacy_users' table to a pandas DataFrame.\n",
    "user_table_rds = 'legacy_users'\n",
    "extractor_rds = DataExtractor(connection_rds)\n",
    "user_df_rds = extractor_rds.read_rds_table(user_table_rds)\n",
    "print(user_df_rds.head(5))\n",
    "print(user_df_rds.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.Cleaning data\n",
    "clean_data_rds = DataCleaning()\n",
    "clean_df_rds = clean_data_rds.clean_user_data(user_df_rds)\n",
    "clean_dob = clean_data_rds.date_formatting(clean_df_rds, 'date_of_birth')\n",
    "\n",
    "clean_join_date = clean_data_rds.date_formatting(clean_dob,'join_date')\n",
    "clean_email = clean_data_rds.email_clean(clean_join_date, 'email_address')\n",
    "clean_first_name = clean_data_rds.gibberish_clean(clean_email, 'first_name')\n",
    "clean_last_name = clean_data_rds.gibberish_clean(clean_first_name, 'last_name')\n",
    "clean_dim_users = clean_data_rds.null_replace(clean_last_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Loading the data\n",
    "#load clean rds dataframe into local database with a new table\n",
    "uploader.upload_to_db(uploading, clean_df_rds, 'dim_users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF ETL: Card Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.Getting the data from PDF & load in df\n",
    "connector_pdf = DatabaseConnector()\n",
    "extractor_pdf = DataExtractor(connector_pdf)\n",
    "\n",
    "link = \"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\"\n",
    "extraction_pdf = extractor_pdf.retrieve_pdf_data(link)\n",
    "print(extraction_pdf.head(5))\n",
    "print(extraction_pdf.info())\n",
    "\n",
    "### 2.Clean the pdf df\n",
    "\n",
    "clean_pdf = DataCleaning()\n",
    "\n",
    "clean_df_pdf =clean_pdf.clean_card_data(extraction_pdf)\n",
    "\n",
    "### 3. load into db\n",
    "\n",
    "uploader.upload_to_db(uploading, clean_df_pdf, 'dim_card_details')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API ETL:  Store\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'statusCode': 200, 'number_stores': 451}\n"
     ]
    }
   ],
   "source": [
    "### 1. Getting Data from API\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'x-api-key': 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'\n",
    "}\n",
    "\n",
    "number_of_stores_endpoint = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores'\n",
    "store_details_endpoint = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{store_number}'\n",
    "\n",
    "connector_api = DatabaseConnector()\n",
    "extractor_api = DataExtractor(connector_api)\n",
    "\n",
    "\n",
    "number_of_stores = extractor_api.list_number_of_stores(number_of_stores_endpoint, headers)\n",
    "print(number_of_stores)\n",
    "\n",
    "\n",
    "### 2. Extracting the table to a pandas DataFrame\n",
    "retrieve_store_endpoint = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/2'\n",
    "\n",
    "stores_df = extractor_api.retrieve_stores_data(retrieve_store_endpoint, headers, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'x-api-key': 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'\n",
    "}\n",
    "\n",
    "number_of_stores_endpoint = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores'\n",
    "store_details_endpoint = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{store_number}'\n",
    "\n",
    "extractor_api = DataExtractor()\n",
    "number_of_stores = extractor_api.list_number_of_stores(number_of_stores_endpoint, headers)\n",
    "print(f\"Number of stores: {number_of_stores}\")\n",
    "\n",
    "stores_df = extractor_api.retrieve_stores_data(store_details_endpoint, headers, number_of_stores)\n",
    "print(stores_df.head())\n",
    "\n",
    "cleaner = DataCleaning()\n",
    "clean_stores_df = cleaner._clean_store_data(stores_df)\n",
    "print(clean_stores_df.head())\n",
    "\n",
    "uploader = DataUploader()\n",
    "uploader.upload_to_db(clean_stores_df, 'dim_store_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.Cleaning data\n",
    "\n",
    "clean_stores_api = DataCleaning()\n",
    "\n",
    "\n",
    "clean_store_date = clean_stores_api.date_formatting(stores_df, 'opening_date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Loading the data\n",
    "uploader.upload_to_db(uploading,clean_store_date, 'dim_store_details')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 ETL: Products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Getting data\n",
    "\n",
    "connector_s3 = DatabaseConnector()\n",
    "extractor_s3 = DataExtractor(connector_s3)\n",
    "\n",
    "\n",
    "### 2. Putting into pd df\n",
    "\n",
    "bucket = 'data-handling-public'\n",
    "object_key = \"products.csv\"\n",
    "pathway = '/Users/student/AICORE/AWS/Project_3/products.csv'\n",
    "\n",
    "extraction_s3 = extractor_s3.download_csv_from_s3(bucket, object_key, pathway)\n",
    "print(extraction_s3.head(20))\n",
    "print(extraction_s3.info())\n",
    "\n",
    "### 3. Cleaning data\n",
    "\n",
    "clean_s3 = DataCleaning()\n",
    "\n",
    "clean_df_s3 = clean_s3.called_clean_store_data(extraction_s3)\n",
    "\n",
    "\n",
    "clean_df_s3 = clean_s3.called_clean_store_data(extraction_s3)\n",
    "\n",
    "\n",
    "converted_df_s3 = clean_s3.convert_product_weights(clean_df_s3)\n",
    "print(converted_df_s3.head())\n",
    "\n",
    "\n",
    "clean_df_s3 = clean_s3.clean_products_data(converted_df_s3)\n",
    "\n",
    "\n",
    "### 4. Loading into local db\n",
    "uploader.upload_to_db(uploading, clean_df_s3, 'dim_products')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS DB ETL: Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector_tables_rds = connector_rds.list_db_tables()\n",
    "\n",
    "#'orders_table'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_table_rds = 'orders_table'\n",
    "\n",
    "extractor_rds = DataExtractor(connection_rds)\n",
    "\n",
    "orders_df_rds = extractor_rds.read_rds_table(orders_table_rds)\n",
    "print(orders_df_rds.head(5))\n",
    "print(orders_df_rds.info())\n",
    "\n",
    "#cleaning the orders table\n",
    "\n",
    "clean_ordersdf_rds = clean_data_rds.clean_user_data(orders_df_rds)\n",
    "\n",
    "## dropping columns\n",
    "\n",
    "ordersdf_rds = clean_ordersdf_rds.drop(['first_name','last_name','1' ], axis = 1)\n",
    "print(ordersdf_rds.info())\n",
    "\n",
    "#uploading the updated orders table into the local db\n",
    "uploader.upload_to_db(uploading, ordersdf_rds, 'orders_table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON S3 ETL: Date Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/student/AICORE/AWS/Project_3/data_extraction.py:68: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_s3 = pd.read_json(content)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   timestamp month  year day time_period                             date_uuid\n",
      "0   22:00:06     9  2012  19     Evening  3b7ca996-37f9-433f-b6d0-ce8391b615ad\n",
      "1   22:44:06     2  1997  10     Evening  adc86836-6c35-49ca-bb0d-65b6507a00fa\n",
      "2   10:05:37     4  1994  15     Morning  5ff791bf-d8e0-4f86-8ceb-c7b60bef9b31\n",
      "3   17:29:27    11  2001   6      Midday  1b01fcef-5ab9-404c-b0d4-1e75a0bd19d8\n",
      "4   22:40:33    12  2015  31     Evening  dfa907c1-f6c5-40f0-aa0d-40ed77ac5a44\n",
      "5   02:03:25     8  2002   2  Late_Hours  cea14ebe-06b2-4cee-8394-972101419375\n",
      "6   20:41:58     1  1993  14     Evening  5b72f016-349c-4649-aea0-bcc4fd1b9ee6\n",
      "7   22:36:19     3  2006  15     Evening  5d598d23-ee71-4fc9-a895-77fd3891a7c4\n",
      "8   18:06:15    12  1994  14     Evening  ad50dbd6-b736-4ffe-a745-6ebed29e4cdf\n",
      "9   16:12:22    11  1994   1      Midday  92fc92a3-c4a0-49a4-a2ee-40a37f9d11dc\n",
      "10  21:21:31    11  1994  21     Evening  21451b39-3bac-4106-b238-8d78c6a08692\n",
      "11  19:23:21     8  2004   3     Evening  08f17ff8-8595-4c23-a0b4-4a16ffd5ec27\n",
      "12  20:46:08    11  2015  16     Evening  940cb999-6d4f-480b-9933-3465353c4caf\n",
      "13  19:59:46     7  2008  25     Evening  9f698beb-8bf6-4b87-8301-3e3579264031\n",
      "14  09:30:07     2  2006  10     Morning  b87d2eb2-8d05-4961-bc27-2ba7d1022d5a\n",
      "15  20:56:48     3  2021  17     Evening  cd6a92c4-77f7-4a61-9665-85d9c3026b35\n",
      "16  23:59:25     7  2018  12     Evening  503ba9b2-99bc-4601-92dc-54ece4ad64b9\n",
      "17  19:13:58    11  2009  23     Evening  b7436486-0d59-46b9-bd92-091fa3fca022\n",
      "18  20:07:37    11  2006  26     Evening  b9018015-cb5b-4f57-abbb-5e2088aaf21b\n",
      "19  16:33:01     9  2020  15      Midday  a360d20d-befd-40f3-90df-bda108a05546\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 120161 entries, 0 to 120160\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   timestamp    120161 non-null  object\n",
      " 1   month        120161 non-null  object\n",
      " 2   year         120161 non-null  object\n",
      " 3   day          120161 non-null  object\n",
      " 4   time_period  120161 non-null  object\n",
      " 5   date_uuid    120161 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 6.4+ MB\n",
      "None\n",
      "Checking for Null values:\n",
      "timestamp      0.0\n",
      "month          0.0\n",
      "year           0.0\n",
      "day            0.0\n",
      "time_period    0.0\n",
      "date_uuid      0.0\n",
      "dtype: float64\n",
      "Deleting rows with more than 4 Null values\n",
      "       timestamp month  year day time_period  \\\n",
      "0       22:00:06     9  2012  19     Evening   \n",
      "1       22:44:06     2  1997  10     Evening   \n",
      "2       10:05:37     4  1994  15     Morning   \n",
      "3       17:29:27    11  2001   6      Midday   \n",
      "4       22:40:33    12  2015  31     Evening   \n",
      "...          ...   ...   ...  ..         ...   \n",
      "120156  22:56:56    11  2022  12     Evening   \n",
      "120157  18:25:20     5  1997  31     Evening   \n",
      "120158  18:21:40     9  2011  13     Evening   \n",
      "120159  19:10:53     7  2013  12     Evening   \n",
      "120160  21:17:12     3  2008  18     Evening   \n",
      "\n",
      "                                   date_uuid  \n",
      "0       3b7ca996-37f9-433f-b6d0-ce8391b615ad  \n",
      "1       adc86836-6c35-49ca-bb0d-65b6507a00fa  \n",
      "2       5ff791bf-d8e0-4f86-8ceb-c7b60bef9b31  \n",
      "3       1b01fcef-5ab9-404c-b0d4-1e75a0bd19d8  \n",
      "4       dfa907c1-f6c5-40f0-aa0d-40ed77ac5a44  \n",
      "...                                      ...  \n",
      "120156  d6c4fb31-720d-4e94-aa6b-dcbcb85f2bb7  \n",
      "120157  f7722027-1aae-49c3-8f8d-853e93f9f3e6  \n",
      "120158  4a3b9851-52e1-463c-ac81-1960f141444e  \n",
      "120159  64974909-0d4b-42a2-822a-73b5695e8bfb  \n",
      "120160  55c228c7-14ee-4d46-99a9-01dfc57d1adf  \n",
      "\n",
      "[120161 rows x 6 columns]\n",
      "Looking for duplicates:\n",
      "0         False\n",
      "1         False\n",
      "2         False\n",
      "3         False\n",
      "4         False\n",
      "          ...  \n",
      "120156    False\n",
      "120157    False\n",
      "120158    False\n",
      "120159    False\n",
      "120160    False\n",
      "Length: 120161, dtype: bool\n",
      "checking data types for each columns\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 120161 entries, 0 to 120160\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   timestamp    120161 non-null  object\n",
      " 1   month        120161 non-null  object\n",
      " 2   year         120161 non-null  object\n",
      " 3   day          120161 non-null  object\n",
      " 4   time_period  120161 non-null  object\n",
      " 5   date_uuid    120161 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 6.4+ MB\n",
      "None\n",
      "Data uploaded successfully to dim_date_times table.\n"
     ]
    }
   ],
   "source": [
    "#Preparing the extration\n",
    "\n",
    "connector_json_s3 = DatabaseConnector()\n",
    "extractor_json_s3 = DataExtractor(connector_json_s3)\n",
    "bucket = 'data-handling-public'\n",
    "object_key_json = \"date_details.json\"\n",
    "\n",
    "#Get the object\n",
    "\n",
    "extraction_json_s3 = extractor_json_s3.extract_json_from_s3(bucket, object_key_json)\n",
    "print(extraction_json_s3.head(20))\n",
    "print(extraction_json_s3.info())\n",
    "\n",
    "#Clean the data\n",
    "clean_json_s3 = DataCleaning()\n",
    "\n",
    "clean_df_json_s3 = clean_json_s3.called_clean_store_data(extraction_json_s3)\n",
    "clean_df_date = clean_json_s3.null_replace(extraction_json_s3)\n",
    "\n",
    "#upload to local db\n",
    "uploader.upload_to_db(uploading, clean_df_json_s3, 'dim_date_times')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

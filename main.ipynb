{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules and staring local db engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing modules\n",
    "\n",
    "import boto3\n",
    "\n",
    "from data_cleaning import DataCleaning\n",
    "from data_extraction import DataExtractor\n",
    "from database_utils import DatabaseConnector\n",
    "import importlib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "import tabula\n",
    "import yaml\n",
    "\n",
    "\n",
    "#starting local database\n",
    "\n",
    "#read creds\n",
    "uploader = DatabaseConnector()\n",
    "ml_yaml = 'ml_dbs.yaml'\n",
    "uploader.read_db_creds(ml_yaml)\n",
    "\n",
    "# initialising and returning an sqlalchemy database engine.\n",
    "\n",
    "uploading = uploader.init_db_engine_postgresql()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDS ETL: User\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Getting data from the RDS database\n",
    "\n",
    "## read credentials\n",
    "connector_rds = DatabaseConnector()\n",
    "ai_core_yaml = 'db_creds.yaml'\n",
    "connector_rds.read_db_creds(ai_core_yaml)\n",
    "\n",
    "## initialising and returning an sqlalchemy database engine.\n",
    "connection_rds = connector_rds.init_db_engine()\n",
    "\n",
    "## Reading the data from the RDS database\n",
    "connector_tables_rds = connector_rds.list_db_tables()\n",
    "\n",
    "\n",
    "### 2. Extracting the 'legacy_users' table to a pandas DataFrame.\n",
    "user_table_rds = 'legacy_users'\n",
    "extractor_rds = DataExtractor(connection_rds)\n",
    "user_df_rds = extractor_rds.read_rds_table(user_table_rds)\n",
    "print(user_df_rds.head(5))\n",
    "print(user_df_rds.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.Cleaning data\n",
    "clean_data_rds = DataCleaning()\n",
    "clean_df_rds = clean_data_rds.clean_user_data(user_df_rds)\n",
    "clean_dob = clean_data_rds.date_formatting(clean_df_rds, 'date_of_birth')\n",
    "\n",
    "clean_join_date = clean_data_rds.date_formatting(clean_dob,'join_date')\n",
    "clean_email = clean_data_rds.email_clean(clean_join_date, 'email_address')\n",
    "clean_first_name = clean_data_rds.gibberish_clean(clean_email, 'first_name')\n",
    "clean_last_name = clean_data_rds.gibberish_clean(clean_first_name, 'last_name')\n",
    "clean_dim_users = clean_data_rds.null_replace(clean_last_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Loading the data\n",
    "#load clean rds dataframe into local database with a new table\n",
    "uploader.upload_to_db(uploading, clean_df_rds, 'dim_users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF ETL: Card Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.Getting the data from PDF & load in df\n",
    "connector_pdf = DatabaseConnector()\n",
    "extractor_pdf = DataExtractor(connector_pdf)\n",
    "\n",
    "link = \"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\"\n",
    "extraction_pdf = extractor_pdf.retrieve_pdf_data(link)\n",
    "print(extraction_pdf.head(5))\n",
    "print(extraction_pdf.info())\n",
    "\n",
    "### 2.Clean the pdf df\n",
    "\n",
    "clean_pdf = DataCleaning()\n",
    "\n",
    "clean_df_pdf =clean_pdf.clean_card_data(extraction_pdf)\n",
    "\n",
    "### 3. load into db\n",
    "\n",
    "uploader.upload_to_db(uploading, clean_df_pdf, 'dim_card_details')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API ETL:  Store\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data for store number 451. Status code: 500\n",
      "Failed to retrieve data for store number 452. Status code: 500\n",
      "   index                                            address longitude   lat  \\\n",
      "0      1  Flat 72W\\nSally isle\\nEast Deantown\\nE7B 8EB, ...  51.62907  None   \n",
      "1      2        Heckerstraße 4/5\\n50491 Säckingen, Landshut  48.52961  None   \n",
      "2      3  5 Harrison tunnel\\nSouth Lydia\\nWC9 2BE, Westbury     51.26  None   \n",
      "3      4  Studio 6\\nStephen landing\\nSouth Simon\\nB77 2W...   53.0233  None   \n",
      "4      5  Flat 92u\\nChristian harbors\\nPort Charlotte\\nN...  53.38333  None   \n",
      "5      6  7 Gillian rue\\nWest Robertside\\nPH4 8NY, Ruthe...  55.82885  None   \n",
      "6      7  Lilija-Heß-Allee 660\\n34566 Regensburg, Stuttgart  48.78232  None   \n",
      "7      8     510 Jill Mill\\nSouth Laura, FL 38723, Kaukauna  44.27804  None   \n",
      "8      9   3 Lee valleys\\nWest Janetview\\nDY4M 2RL, Hartley  51.38673  None   \n",
      "9     10  8 Gareth skyway\\nSimmonsview\\nSW4 7PL, Rutherglen  55.82885  None   \n",
      "\n",
      "       locality   store_code staff_numbers     opening_date   store_type  \\\n",
      "0  High Wycombe  HI-9B97EE4E            34       1996-10-25        Local   \n",
      "1      Landshut  LA-0772C7B9            92       2013-04-12  Super Store   \n",
      "2      Westbury  WE-1DE82CEE            69       2014-01-02  Super Store   \n",
      "3        Belper  BE-18074576            35       2019-09-09        Local   \n",
      "4  Gainsborough  GA-CAD01AC2            36       1995-05-15        Local   \n",
      "5    Rutherglen  RU-C603E990            92       2001-01-04  Super Store   \n",
      "6     Stuttgart  ST-229D997E            34       2000-06-01        Local   \n",
      "7      Kaukauna  KA-FA7ED3B8            31       2022-09-05        Local   \n",
      "8       Hartley  HA-974352FE            20       2004-09-11        Local   \n",
      "9    Rutherglen  RU-9F1136B4            32  October 2012 08        Local   \n",
      "\n",
      "    latitude country_code continent  \n",
      "0   -0.74934           GB    Europe  \n",
      "1   12.16179           DE    Europe  \n",
      "2    -2.1875           GB    Europe  \n",
      "3   -1.48119           GB    Europe  \n",
      "4   -0.76667           GB    Europe  \n",
      "5   -4.21376           GB    Europe  \n",
      "6    9.17702           DE    Europe  \n",
      "7  -88.27205           US   America  \n",
      "8    0.30367           GB    Europe  \n",
      "9   -4.21376           GB    Europe  \n"
     ]
    }
   ],
   "source": [
    "### 1. Getting Data from API\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'x-api-key': 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'\n",
    "}\n",
    "\n",
    "number_of_stores_endpoint = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores'\n",
    "store_details_endpoint = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{store_number}'\n",
    "\n",
    "connector_api = DatabaseConnector()\n",
    "extractor_api = DataExtractor(connector_api)\n",
    "\n",
    "\n",
    "number_of_stores = extractor_api.list_number_of_stores(number_of_stores_endpoint, headers)\n",
    "print(number_of_stores)\n",
    "\n",
    "\n",
    "### 2. Extracting the table to a pandas DataFrame\n",
    "\n",
    "stores_df = extractor_api.retrieve_stores_data(store_details_endpoint, headers, number_of_stores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stores_df.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.Cleaning data\n",
    "\n",
    "clean_stores_api = DataCleaning()\n",
    "\n",
    "replace_nulls_api = clean_stores_api.null_replace(stores_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 447 entries, 0 to 449\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   index          447 non-null    int64 \n",
      " 1   address        447 non-null    object\n",
      " 2   longitude      447 non-null    object\n",
      " 3   lat            7 non-null      object\n",
      " 4   locality       447 non-null    object\n",
      " 5   store_code     447 non-null    object\n",
      " 6   staff_numbers  447 non-null    object\n",
      " 7   opening_date   447 non-null    object\n",
      " 8   store_type     447 non-null    object\n",
      " 9   latitude       447 non-null    object\n",
      " 10  country_code   447 non-null    object\n",
      " 11  continent      447 non-null    object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 45.4+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 447 entries, 0 to 449\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   index          447 non-null    int64 \n",
      " 1   address        447 non-null    object\n",
      " 2   longitude      447 non-null    object\n",
      " 3   locality       447 non-null    object\n",
      " 4   store_code     447 non-null    object\n",
      " 5   staff_numbers  447 non-null    object\n",
      " 6   opening_date   447 non-null    object\n",
      " 7   store_type     447 non-null    object\n",
      " 8   latitude       447 non-null    object\n",
      " 9   country_code   447 non-null    object\n",
      " 10  continent      447 non-null    object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 41.9+ KB\n",
      "Data uploaded successfully to dim_store_details table.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "drop_df = replace_nulls_api.dropna(axis=0, thresh=2)\n",
    "drop_df_columns = drop_df.dropna(axis = 1, thresh = 100)\n",
    "\n",
    "print(drop_df.info())\n",
    "\n",
    "drop_df_columns.info()\n",
    "\n",
    "### 4. Loading the data\n",
    "uploader.upload_to_db(uploading,drop_df_columns, 'dim_store_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 ETL: Products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Getting data\n",
    "\n",
    "connector_s3 = DatabaseConnector()\n",
    "extractor_s3 = DataExtractor(connector_s3)\n",
    "\n",
    "\n",
    "### 2. Putting into pd df\n",
    "\n",
    "bucket = 'data-handling-public'\n",
    "object_key = \"products.csv\"\n",
    "pathway = '/Users/student/AICORE/AWS/Project_3/products.csv'\n",
    "\n",
    "extraction_s3 = extractor_s3.download_csv_from_s3(bucket, object_key, pathway)\n",
    "print(extraction_s3.head(20))\n",
    "print(extraction_s3.info())\n",
    "\n",
    "### 3. Cleaning data\n",
    "\n",
    "clean_s3 = DataCleaning()\n",
    "\n",
    "clean_df_s3 = clean_s3.called_clean_store_data(extraction_s3)\n",
    "\n",
    "\n",
    "clean_df_s3 = clean_s3.called_clean_store_data(extraction_s3)\n",
    "\n",
    "\n",
    "converted_df_s3 = clean_s3.convert_product_weights(clean_df_s3)\n",
    "print(converted_df_s3.head())\n",
    "\n",
    "\n",
    "clean_df_s3 = clean_s3.clean_products_data(converted_df_s3)\n",
    "\n",
    "\n",
    "### 4. Loading into local db\n",
    "uploader.upload_to_db(uploading, clean_df_s3, 'dim_products')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS DB ETL: Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector_tables_rds = connector_rds.list_db_tables()\n",
    "\n",
    "#'orders_table'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_table_rds = 'orders_table'\n",
    "\n",
    "extractor_rds = DataExtractor(connection_rds)\n",
    "\n",
    "orders_df_rds = extractor_rds.read_rds_table(orders_table_rds)\n",
    "print(orders_df_rds.head(5))\n",
    "print(orders_df_rds.info())\n",
    "\n",
    "#cleaning the orders table\n",
    "\n",
    "clean_ordersdf_rds = clean_data_rds.clean_user_data(orders_df_rds)\n",
    "\n",
    "## dropping columns\n",
    "\n",
    "ordersdf_rds = clean_ordersdf_rds.drop(['first_name','last_name','1' ], axis = 1)\n",
    "print(ordersdf_rds.info())\n",
    "\n",
    "#uploading the updated orders table into the local db\n",
    "uploader.upload_to_db(uploading, ordersdf_rds, 'orders_table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON S3 ETL: Date Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the extration\n",
    "\n",
    "connector_json_s3 = DatabaseConnector()\n",
    "extractor_json_s3 = DataExtractor(connector_json_s3)\n",
    "bucket = 'data-handling-public'\n",
    "object_key_json = \"date_details.json\"\n",
    "\n",
    "#Get the object\n",
    "\n",
    "extraction_json_s3 = extractor_json_s3.extract_json_from_s3(bucket, object_key_json)\n",
    "print(extraction_json_s3.head(20))\n",
    "print(extraction_json_s3.info())\n",
    "\n",
    "#Clean the data\n",
    "clean_json_s3 = DataCleaning()\n",
    "\n",
    "clean_df_json_s3 = clean_json_s3.called_clean_store_data(extraction_json_s3)\n",
    "clean_df_date = clean_json_s3.null_replace(extraction_json_s3)\n",
    "\n",
    "#upload to local db\n",
    "uploader.upload_to_db(uploading, clean_df_json_s3, 'dim_date_times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Local', 'Super Store', 'Mall Kiosk', 'Outlet', 'QP74AHEQT0',\n",
       "       'O0QJIRC943', 'NULL', '50IB01SFAZ', '0RSNUU3DF5', 'B4KVQB3P5Y',\n",
       "       'X0FE7E2EOG', 'NN04B3F6UQ'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#stores_df.store_type.unique()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

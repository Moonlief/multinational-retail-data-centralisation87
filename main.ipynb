{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules and staring local db engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing modules\n",
    "\n",
    "import boto3\n",
    "\n",
    "from data_cleaning import DataCleaning\n",
    "from data_extraction import DataExtractor\n",
    "from database_utils import DatabaseConnector\n",
    "import importlib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "import tabula\n",
    "import yaml\n",
    "\n",
    "\n",
    "#starting local database\n",
    "\n",
    "#read creds\n",
    "uploader = DatabaseConnector()\n",
    "ml_yaml = 'ml_dbs.yaml'\n",
    "uploader.read_db_creds(ml_yaml)\n",
    "\n",
    "# initialising and returning an sqlalchemy database engine.\n",
    "\n",
    "uploading = uploader.init_db_engine_postgresql()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDS ETL: User\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Getting data from the RDS database\n",
    "\n",
    "## read credentials\n",
    "connector_rds = DatabaseConnector()\n",
    "ai_core_yaml = 'db_creds.yaml'\n",
    "connector_rds.read_db_creds(ai_core_yaml)\n",
    "\n",
    "## initialising and returning an sqlalchemy database engine.\n",
    "connection_rds = connector_rds.init_db_engine()\n",
    "\n",
    "## Reading the data from the RDS database\n",
    "connector_tables_rds = connector_rds.list_db_tables()\n",
    "\n",
    "\n",
    "### 2. Extracting the 'legacy_users' table to a pandas DataFrame.\n",
    "user_table_rds = 'legacy_users'\n",
    "extractor_rds = DataExtractor(connection_rds)\n",
    "user_df_rds = extractor_rds.read_rds_table(user_table_rds)\n",
    "print(user_df_rds.head(5))\n",
    "print(user_df_rds.info())\n",
    "\n",
    "### 3.Cleaning data\n",
    "clean_data_rds = DataCleaning()\n",
    "clean_df_rds = clean_data_rds.clean_user_data(user_df_rds)\n",
    "\n",
    "### 4. Loading the data\n",
    "#load clean rds dataframe into local database with a new table\n",
    "uploader.upload_to_db(uploading, clean_df_rds, 'dim_users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF ETL: Card Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.Getting the data from PDF & load in df\n",
    "connector_pdf = DatabaseConnector()\n",
    "extractor_pdf = DataExtractor(connector_pdf)\n",
    "\n",
    "link = \"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\"\n",
    "extraction_pdf = extractor_pdf.retrieve_pdf_data(link)\n",
    "print(extraction_pdf.head(5))\n",
    "print(extraction_pdf.info())\n",
    "\n",
    "### 2.Clean the pdf df\n",
    "\n",
    "clean_pdf = DataCleaning()\n",
    "\n",
    "clean_df_pdf =clean_pdf.clean_card_data(extraction_pdf)\n",
    "\n",
    "### 3. load into db\n",
    "\n",
    "uploader.upload_to_db(uploading, clean_df_pdf, 'dim_card_details')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API ETL:  Store\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Getting Data from API\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'x-api-key': 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'\n",
    "}\n",
    "\n",
    "number_of_stores_endpoint = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores'\n",
    "\n",
    "connector_api = DatabaseConnector()\n",
    "\n",
    "extractor_api = DataExtractor(connector_api)\n",
    "\n",
    "number_of_stores = extractor_api.list_number_of_stores(number_of_stores_endpoint, headers)\n",
    "print(f\"Number of stores: {number_of_stores}\")\n",
    "\n",
    "### 2. Extracting the table to a pandas DataFrame\n",
    "retrieve_store_endpoint = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/2'\n",
    "\n",
    "stores_df = extractor_api.retrieve_stores_data(retrieve_store_endpoint, headers, 2)\n",
    "\n",
    "### 3.Cleaning data\n",
    "\n",
    "clean_stores_api = DataCleaning()\n",
    "clean_storesdf_api = clean_stores_api.called_clean_store_data(stores_df)\n",
    "\n",
    "### 4. Loading the data\n",
    "uploader.upload_to_db(uploading, clean_storesdf_api, 'dim_store_details')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 ETL: Products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Getting data\n",
    "\n",
    "connector_s3 = DatabaseConnector()\n",
    "extractor_s3 = DataExtractor(connector_s3)\n",
    "\n",
    "\n",
    "### 2. Putting into pd df\n",
    "\n",
    "bucket = 'data-handling-public'\n",
    "object_key = \"products.csv\"\n",
    "pathway = '/Users/student/AICORE/AWS/Project_3/products.csv'\n",
    "\n",
    "extraction_s3 = extractor_s3.extract_from_s3(bucket, object_key, pathway)\n",
    "print(extraction_s3.head(20))\n",
    "print(extraction_s3.info())\n",
    "\n",
    "### 3. Cleaning data\n",
    "\n",
    "clean_s3 = DataCleaning()\n",
    "\n",
    "clean_df_s3 = clean_s3.called_clean_store_data(extraction_s3)\n",
    "\n",
    "\n",
    "clean_df_s3 = clean_s3.called_clean_store_data(extraction_s3)\n",
    "\n",
    "\n",
    "converted_df_s3 = clean_s3.convert_product_weights(clean_df_s3)\n",
    "print(converted_df_s3.head())\n",
    "\n",
    "\n",
    "clean_df_s3 = clean_s3.clean_products_data(converted_df_s3)\n",
    "\n",
    "\n",
    "### 4. Loading into local db\n",
    "uploader.upload_to_db(uploading, clean_df_s3, 'dim_products')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS DB ETL: Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector_tables_rds = connector_rds.list_db_tables()\n",
    "\n",
    "#'orders_table'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_table_rds = 'orders_table'\n",
    "\n",
    "extractor_rds = DataExtractor(connection_rds)\n",
    "\n",
    "orders_df_rds = extractor_rds.read_rds_table(orders_table_rds)\n",
    "print(orders_df_rds.head(5))\n",
    "print(orders_df_rds.info())\n",
    "\n",
    "#cleaning the orders table\n",
    "\n",
    "clean_ordersdf_rds = clean_data_rds.clean_user_data(orders_df_rds)\n",
    "\n",
    "## dropping columns\n",
    "\n",
    "ordersdf_rds = clean_ordersdf_rds.drop(['first_name','last_name','1' ], axis = 1)\n",
    "print(ordersdf_rds.info())\n",
    "\n",
    "#uploading the updated orders table into the local db\n",
    "uploader.upload_to_db(uploading, ordersdf_rds, 'orders_table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON S3 ETL: Date Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:863\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m object_key_json\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_details.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m pathway_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/student/AICORE/AWS/Project_3/date_details.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m extraction_json \u001b[38;5;241m=\u001b[39m \u001b[43mextractor_json\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_from_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_key_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpathway_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(extraction_json\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(extraction_json\u001b[38;5;241m.\u001b[39minfo())\n",
      "File \u001b[0;32m~/AICORE/AWS/Project_3/data_extraction.py:60\u001b[0m, in \u001b[0;36mDataExtractor.extract_from_s3\u001b[0;34m(self, s3_bucket, s3_object_key, local_file_path)\u001b[0m\n\u001b[1;32m     58\u001b[0m s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m s3\u001b[38;5;241m.\u001b[39mdownload_file(s3_bucket, s3_object_key, local_file_path)\n\u001b[0;32m---> 60\u001b[0m df_csv \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_object_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m df_s3 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df_csv)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_s3\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:256\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_usecols(columns)\n\u001b[0;32m--> 256\u001b[0m     col_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m col_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns}\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m index, columns, col_dict\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## JSON\n",
    "\n",
    "connector_json = DatabaseConnector()\n",
    "extractor_json = DataExtractor(connector_json)\n",
    "bucket = 'data-handling-public'\n",
    "object_key_json= \"date_details.json\"\n",
    "pathway_json = '/Users/student/AICORE/AWS/Project_3/date_details.json'\n",
    "\n",
    "extraction_json = extractor_json.extract_from_s3(bucket, object_key_json, pathway_json)\n",
    "print(extraction_json.head(20))\n",
    "print(extraction_json.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df_s3 = clean_s3.clean_products_data(converted_df_s3)\n",
    "\n",
    "#dim_date_times\n",
    "\n",
    "#uploader.upload_to_db(uploading, clean_df_s3, 'dim_date_times')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
